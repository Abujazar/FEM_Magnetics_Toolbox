    # == Adaptive Meshing ==
    # !!! Experimental Code !!!
    def triangle_max_edge(self, x):
        a = np.sum((x[:, 0, :] - x[:, 1, :]) ** 2, 1) ** 0.5
        b = np.sum((x[:, 0, :] - x[:, 2, :]) ** 2, 1) ** 0.5
        c = np.sum((x[:, 1, :] - x[:, 2, :]) ** 2, 1) ** 0.5
        return np.maximum(a, np.maximum(b, c))

    def triangle_mean_edge(self, x):
        a = np.sum((x[:, 0, :] - x[:, 1, :]) ** 2, 1) ** 0.5
        b = np.sum((x[:, 0, :] - x[:, 2, :]) ** 2, 1) ** 0.5
        c = np.sum((x[:, 1, :] - x[:, 2, :]) ** 2, 1) ** 0.5
        return (a + b + c)/3

    def compute_size_field_old(self, nodes, triangles, err, N):
        x = nodes[triangles]
        a = 2.
        d = 2.
        fact = (a ** ((2. + a) / (1. + a)) + a ** (1. / (1. + a))) * np.sum(err ** (2. / (1. + a)))
        ri = err ** (2. / (2. * (1 + a))) * a ** (1. / (d * (1. + a))) * ((1. + a) * N / fact) ** (1. / d)
        return self.triangle_max_edge(x) / ri

    def compute_size_field(self, nodes, triangles, err, N):
        x = nodes[triangles]
        threshold = 0.01
        print(f"Error{err}")
        err[err > 0.1] = 0.5
        err[err < 0.1] = 0.9
        print(f"Error{err}")
        return self.triangle_max_edge(x) * err
        # return self.triangle_max_edge(x) - err**(0.1) * self.triangle_max_edge(x)

    class Mesh:
        """
        Currently unused except for experimental adaptive Meshing.
        #TODO: Make the mesh an object for increased reusability
        """
        def __init__(self):
            self.vtags, vxyz, _ = gmsh.model.mesh.getNodes()
            self.vxyz = vxyz.reshape((-1, 3))
            vmap = dict({j: i for i, j in enumerate(self.vtags)})
            self.triangles_tags, evtags = gmsh.model.mesh.getElementsByType(2)
            evid = np.array([vmap[j] for j in evtags])
            self.triangles = evid.reshape((self.triangles_tags.shape[-1], -1))

    def refine_mesh(self, local=0):
        """

        :return:
        """

        # --------------------------------------
        if local == 1:
            self.generate_mesh(refine=1)

        # --------------------------------------
        if local == 0:
            # Refine current mesh
            gmsh.model.mesh.refine()
            print("\nMeshing...\n")
            gmsh.model.mesh.generate(2)
            # --------------------------------------
            # Mesh generation
            #gmsh.model.mesh.generate(2)
            # Check operating system
            if sys.platform == "linux" or sys.platform == "linux2":
                gmsh.write(self.path + "/" + self.path_mesh + "geometry.msh")
            elif sys.platform == "darwin":
                # OS X
                gmsh.write(self.path + "/" + self.path_mesh + "geometry.msh")
            elif sys.platform == "win32":
                gmsh.write(self.path + "/" + self.path_mesh + "geometry.msh")  # Win10 can handle slash

            # Terminate gmsh
            gmsh.finalize()

    def find_neighbours(self, file="results/J_rms.pos"):
        # Open loss/error results
        dest_file = open(self.path + "error.dat", "w")
        # Read the logged losses corresponding to the frequencies
        with open(self.path + "/" + self.path_res_fields + 'J_rms.pos') as f:
            read = 0
            for line in f:
                if line == "$ElementNodeData\n":
                    read = (read + 1) % 2
                    print(line, read)
                if read == 1 and ' ' in line:
                    # words = line.split(sep=' ')
                    words = line.replace(' ', ', ')
                    for word in words:
                        dest_file.write(word)
        dest_file.close()

    def alternative_local_error(self, loss_file='/results/fields/J_rms.pos'):
        """

        :return:
        """
        # Open loss/error results
        error_file = open(self.path + "/mesh_error.dat", "w")
        # Read the logged losses corresponding to the frequencies
        with open(self.path + loss_file) as f:
            read = 0
            for line in f:
                if line == "$ElementNodeData\n":
                    read = (read + 1) % 2
                    print(line, read)
                if read == 1 and ' ' in line:
                    # words = line.split(sep=' ')
                    words = line.replace(' ', ', ')
                    for word in words:
                        error_file.write(word)
        error_file.close()

        # Load local Error values
        data = pd.read_csv(self.path + "/mesh_error.dat")
        local_error = data.iloc[:, 2].to_numpy()
        local_error = np.insert(local_error, 0, 0., axis=0)

        local_error = local_error / np.max(local_error) + 0.001
        print(f"LÃ¤ngen: {len(local_error), local_error} ")  # first of the 3 loss values

        # ---- Open post-processing results ----

        # Elements ----
        elements = []
        values = []
        # error_file = open(self.path + "/mesh_error.dat", "w")
        # Read the logged losses corresponding to the frequencies
        with open(self.path + "/" + self.path_res_fields + 'J_rms.pos') as file:
            read = 0
            for line in file:
                if line == "$Elements\n" or line == "$EndElements\n":
                    read = (read + 1) % 2
                    print(line, read)
                if read == 1 and ' ' in line:
                    words = line.split(sep=' ')
                    elements.append(words)

        # Convert Elements to Dataframe
        element_frame = pd.DataFrame(elements)
        # Dropout not needed columns
        element_frame.drop(element_frame.columns[[1, 2, 3, 4, 8]], axis=1, inplace=True)
        element_frame.columns = ['NumElement', 'Node1', 'Node2', 'Node3']
        print(f"Number of Elements: {len(elements)}\n"
              # f"Elements: {elements}\n"
              f"Element Dataframe: {element_frame}")
        element_frame.to_csv(path_or_buf="elements.txt")

        # Values ----
        with open(self.path + "/" + self.path_res_fields + 'J_rms.pos') as file:
            read = 0
            for line in file:
                if line == "$ElementNodeData\n":
                    read = (read + 1) % 2
                    print(line, read)
                if read == 1 and ' ' in line:
                    words = re.split(' |\n', line)
                    values.append(words)

        # Convert Values to Dataframe
        value_frame = pd.DataFrame(values)
        # Dropout not needed columns
        value_frame.drop(value_frame.columns[[1, 5]], axis=1, inplace=True)
        value_frame.columns = ['NumElement', 'Node1', 'Node2', 'Node3']
        # value_frame['NumElement', 'Node1', 'Node2', 'Node3'] = pd.to_numeric(value_frame['NumElement', 'Node1', 'Node2', 'Node3'], downcast="float")
        value_frame['NumElement'] = pd.to_numeric(value_frame['NumElement'], downcast="float")
        value_frame['Node1'] = pd.to_numeric(value_frame['Node1'], downcast="float")
        value_frame['Node2'] = pd.to_numeric(value_frame['Node2'], downcast="float")
        value_frame['Node3'] = pd.to_numeric(value_frame['Node3'], downcast="float")
        print(f"Number of Values: {len(values)}\n"
              # f"Values: {values}\n"
              f"Values Dataframe: {value_frame}")

        # ---- Neighbour algorithm || Error calculation ----
        local_error = np.zeros(len(element_frame.index))

        nodes = ['Node1', 'Node2', 'Node3']
        for i in value_frame.index:
            mean_cell = 0
            # Mean loss per cell
            for node in nodes:
                mean_cell += value_frame[node][i] / len(nodes)
            # Local Variance
            for node in nodes:
                local_error[i] += (value_frame[node][i] - mean_cell) ** 2

        # Distribute Local Error on neighbour cells
        nodes_neighbours_found = []
        distribution_factor = 2

        """
        local_error_copy = local_error
        print(local_error)

        # every element
        for i in element_frame.index:
            # every element's node
            for node in nodes:
                # Node already considered?
                if not element_frame[node][i] in nodes_neighbours_found:
                    nodes_neighbours_found += element_frame[node][i]
                    # Value[Node] == 0 ? : skip
                    if not value_frame[node][i] == 0:
                        # search in every element for Node
                        for j in element_frame.index:
                            for node in nodes:
                                if value_frame[node][j] == 0:
                                    # add some error in neighboured Nodes
                                    if element_frame[node][j] == element_frame[node][i]:
                                        local_error_copy[j] += local_error[i] * distribution_factor


        local_error = local_error_copy
        """
        print(local_error)
        # Error Normalization
        return local_error / np.max(local_error) + 0.0001

        # print(f"Local Error: {local_error[3387]}\n"
        #      f"Length of Local Error: {len(local_error)}")
        """
        # Load local Error values
        data = pd.read_csv(self.path + "/mesh_error.dat")
        local_error = data.iloc[:, 2].to_numpy()
        local_error = np.insert(local_error, 0, 0., axis=0)

        local_error = local_error/np.max(local_error) + 0.001
        print(len(local_error), local_error)  # first of the 3 loss values
        """

    def local_error(self, loss_file='/results/fields/error.pos'):
        """
        - Method shall return the normalized numeric local error of the last adaptive simulation step
        - Local error can be used to optimize the mesh in the next iteration step
        :return:
        """
        # Open loss/error results
        error_file = open(self.path + "/mesh_error.dat", "w")
        # Read the logged losses corresponding to the frequencies
        with open(self.path + loss_file) as f:
            read = 0
            for line in f:
                if line == "$ElementNodeData\n":
                    read = (read + 1) % 2
                    print(line, read)
                if read == 1 and ' ' in line:
                    # words = line.split(sep=' ')
                    words = line.replace(' ', ', ')
                    for word in words:
                        error_file.write(word)
        error_file.close()

        # Load local Error values
        data = pd.read_csv(self.path + "/mesh_error.dat")
        print(f"Data: {data}")
        local_error = data.iloc[:, 2].to_numpy()
        local_error = np.insert(local_error, 0, 1e-5, axis=0)

        return local_error / np.max(local_error)
        print(f"LÃ¤ngen: {len(local_error), local_error} ")

    def create_background_mesh(self, local_error):
        gmsh.open(self.path + "/" + self.path_mesh + "geometry.msh")  # Open current mesh
        N = 50000  # Number of elements after remeshing
        mesh = self.Mesh()  # Create virtual mesh
        """
        print(f"Mesh nodes: {mesh.vxyz} \n "
              f"Mesh nodes.shape: {mesh.vxyz.shape} \n "
              f"Mesh node tags: {mesh.vtags} \n"
              f"Mesh triangles: {mesh.triangles} \n"
              f"Mesh triangles.shape: {mesh.triangles.shape} \n"
              f"Mesh triangle tags: {mesh.triangles_tags} \n"
              )
        """  # some printing options

        err_view = gmsh.view.add("element-wise error")
        gmsh.view.addModelData(err_view, 0, self.path_mesh + "/" + self.path_mesh + "geometry", "ElementData", mesh.triangles_tags, local_error[:, None])
        gmsh.view.write(err_view, "err.pos")

        # Refinement
        sf_ele = self.compute_size_field(mesh.vxyz, mesh.triangles, local_error, N)
        np.savetxt("sf_ele.txt", sf_ele)
        sf_view = gmsh.view.add("mesh size field")
        gmsh.view.addModelData(sf_view, 0, self.path_mesh + "/" + self.path_mesh + "geometry", "ElementData", mesh.triangles_tags, sf_ele[:, None])
        gmsh.view.write(sf_view, "sf.pos")

